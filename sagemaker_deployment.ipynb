{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploying a Model Using Amazon SageMaker\n",
    "\n",
    "## Project\n",
    "\n",
    "This is going to be my first end to end machine learning project. I will pre-process the data and develop a model, as I did in every of my previous ML projects. But this time I will also deploy the model to the web, so it can be used by people. The model and the web application are going to be simple as my focus is mainly on doing it end to end using AWS and Amazon Sagemaker.\n",
    "\n",
    "The end result is going to be a simple web app that, given movie review, will tell you if the person likes the movie or not. I stick with a basic RNN for the modelling part.\n",
    "\n",
    "You can treat is as a tutorial and just read it or even execute the project on your own, by uploading the code to SageMaker and following this notebook step by step. I covered here all the steps. So if you want, open it on AWS and let's start.\n",
    "\n",
    "#### Opening notebook instance in Amazon SageMaker\n",
    "\n",
    "The notebook uses `ml.m4.xlarge` instance. You need to make sure that your limit is set to at least `1`. If not, create a case in AWS support to increase the limit.\n",
    "\n",
    "Then you can go to 'Amazon SageMaker' service, from which you choose 'Create notebook instance'. In the creator you need to configure a few settings. For name, choose whatever you want. Notebook instance type can be left as a default `ml.t2.medium`. Under role choose 'Create a new role'. Then just select 'None' under 'S3 buckets you specify' and create a role. Now you can scroll to the bottom and create a notebook instance.\n",
    "\n",
    "Once it is created, you can open and then open a terminal in it. Next, enter `SageMaker` directory and clone this github repository. Now you have everything set up.\n",
    "\n",
    "## Outline\n",
    "\n",
    "The complete machine learning workflow, using jupyter notebook within SageMaker, consists  of the following steps:\n",
    "\n",
    "1. Download the data.\n",
    "2. Prepare it for the model.\n",
    "3. Copy the data to S3.\n",
    "4. Train the model.\n",
    "5. Test it (using a batch transform job).\n",
    "6. Deploy it.\n",
    "7. Use the deployed model.\n",
    "\n",
    "I am going to follow these steps with some modifications. In the testing step, I will deploy the model for testing. It will let me to check not only if a pure model works as expected but also if I am deploying it correctly. In the next step, I will deploy it again. But this time integrating it with a web page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Download the data\n",
    "\n",
    "I am going to download the [IMDb dataset](http://ai.stanford.edu/~amaas/data/sentiment/) with reviews from the original page. The data is compressed, so then I extract it to a data directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘./data’: File exists\n",
      "--2020-11-03 22:19:00--  http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
      "Resolving ai.stanford.edu (ai.stanford.edu)... 171.64.68.10\n",
      "Connecting to ai.stanford.edu (ai.stanford.edu)|171.64.68.10|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 84125825 (80M) [application/x-gzip]\n",
      "Saving to: ‘./data/aclImdb_v1.tar.gz’\n",
      "\n",
      "../data/aclImdb_v1. 100%[===================>]  80.23M  9.72MB/s    in 12s     \n",
      "\n",
      "2020-11-03 22:19:12 (6.83 MB/s) - ‘./data/aclImdb_v1.tar.gz’ saved [84125825/84125825]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%mkdir ./data\n",
    "!wget -O ./data/aclImdb_v1.tar.gz http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "!tar -zxf ./data/aclImdb_v1.tar.gz -C ./data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Prepare it for the model\n",
    "\n",
    "The reviews are just text files. They are already divided into directories *test* and *train* and each of them is divided further into *neg* and *pos* subdirectories. What you would like to do next is to read them in memory. I am going to read all reviews into a dataframe so I can keep the information about split and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "def read_imdb_data(data_dir = '../data/aclImdb'):\n",
    "    data = {}\n",
    "    labels = {}\n",
    "    \n",
    "    for data_type in ['train', 'test']:\n",
    "        data[data_type] = {}\n",
    "        labels[data_type] = {}\n",
    "        \n",
    "        for sentiment in ['pos', 'neg']:\n",
    "            data[data_type][sentiment] = []\n",
    "            labels[data_type][sentiment] = []\n",
    "            \n",
    "            path = os.path.join(data_dir, data_type, sentiment, '*.txt')\n",
    "            files = glob.glob(path)\n",
    "            \n",
    "            for f in files:\n",
    "                with open(f) as review:\n",
    "                    data[data_type][sentiment].append(review.read())\n",
    "                    # Represent a positive review by '1' and a negative review by '0'\n",
    "                    labels[data_type][sentiment].append(1 if sentiment == 'pos' else 0)\n",
    "                    \n",
    "            assert len(data[data_type][sentiment]) == len(labels[data_type][sentiment]), \\\n",
    "                    \"{}/{} data size does not match labels size\".format(data_type, sentiment)\n",
    "                \n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMDB reviews: train = 12500 pos / 12500 neg, test = 12500 pos / 12500 neg\n"
     ]
    }
   ],
   "source": [
    "data, labels = read_imdb_data()\n",
    "print(\"IMDB reviews: train = {} pos / {} neg, test = {} pos / {} neg\".format(\n",
    "            len(data['train']['pos']), len(data['train']['neg']),\n",
    "            len(data['test']['pos']), len(data['test']['neg'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What you actually need for training are complete datasets. I will join positive and negative reviews together and shuffle them for each split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "def prepare_imdb_data(data, labels):\n",
    "    \"\"\"Prepare training and test sets from IMDb movie reviews.\"\"\"\n",
    "    \n",
    "    #Combine positive and negative reviews and labels\n",
    "    data_train = data['train']['pos'] + data['train']['neg']\n",
    "    data_test = data['test']['pos'] + data['test']['neg']\n",
    "    labels_train = labels['train']['pos'] + labels['train']['neg']\n",
    "    labels_test = labels['test']['pos'] + labels['test']['neg']\n",
    "    \n",
    "    #Shuffle reviews and corresponding labels within training and test sets\n",
    "    data_train, labels_train = shuffle(data_train, labels_train)\n",
    "    data_test, labels_test = shuffle(data_test, labels_test)\n",
    "    \n",
    "    # Return a unified training data, test data, training labels, test labets\n",
    "    return data_train, data_test, labels_train, labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMDb reviews (combined): train = 25000, test = 25000\n"
     ]
    }
   ],
   "source": [
    "train_X, test_X, train_y, test_y = prepare_imdb_data(data, labels)\n",
    "print(\"IMDb reviews (combined): train = {}, test = {}\".format(len(train_X), len(test_X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do a quick check and see an example of the data the model will be trained on. This is generally a good idea as it allows you to see how each of the further processing steps affects the reviews and it also ensures that the data has been loaded correctly.\n",
    "\n",
    "I'm going to print 100th review and a corresponding label. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I had to give this film a 10 simply because it did what so many films thrown at black audiences have FAILED MISERABLY to do. This film was void of all the video whore clichés, and it also skipped the gangsters, rappers, and foul language. It examined several relationships among physicians, African Americans, and African American male/female romantic relationships on a completely new and refreshing level. I was highly impressed with the films careful mix of light headed humor with some pretty tough and heavy issues. The film will leave you feeling happy and sad all at the same time. I saw it at the Boston Film Festival as well. It premiered at AMC Loews Boston and I truly hope this film makes it to much larger audiences. Black People (and EVERYONE else) would LOVE a movie like this- if only the industry was SMART enough to put them out there!<br /><br />As an extra- if your a doctor, a resident, a medical student, a premedical student, married to a doctor, have a doctor sibling, have a doctor in the family, or know a great doctor period- they would love this film as well. It portrayed residency and the practice in a true-to-life way that was greatly appreciated by the doctors, residents, and medical students who viewed the film. Dennis Cooper completely his residency in internal medicine- so he definitely applied his knowledge/experience to the film and that is greatly apparent.\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(train_X[100])\n",
    "print(train_y[100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result looks as expected. There is a single review and a single label. 1 means that the review is positive and if you read it, it actually is.\n",
    "\n",
    "When you read the review, you can notice that it contains some html tags like `<br />`. The first step in processing the data will be to remove them. In addition, you wish to tokenize the vocabulary. It simply means that words such as *entertained* and *entertaining* are brought to a common form like *entertain*. Thanks to this, they will be considered the same during analysis. And they should be as the word ending brings no additional information about the text's sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def review_to_words(review):\n",
    "    nltk.download('stopwords', quiet = True)\n",
    "    stemmer = PorterStemmer()\n",
    "    \n",
    "    text = BeautifulSoup(review, 'html.parser').get_text() # Remove HTML tags\n",
    "    text = re.sub(r'[^a-zA-Z0-9]', ' ', text.lower()) # Convert to lower case\n",
    "    words = text.split() # Split string into words\n",
    "    words = [w for w in words if w not in stopwords.words('english')] # Remove stopwords\n",
    "    words = [PorterStemmer().stem(w) for w in words] # stem\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `review_to_words` method defined above uses `BeautifulSoup` to remove any html tags that appear and uses the `nltk` package to tokenize the reviews. Let's test it and apply it to a single, 100th review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['give',\n",
       " 'film',\n",
       " '10',\n",
       " 'simpli',\n",
       " 'mani',\n",
       " 'film',\n",
       " 'thrown',\n",
       " 'black',\n",
       " 'audienc',\n",
       " 'fail',\n",
       " 'miser',\n",
       " 'film',\n",
       " 'void',\n",
       " 'video',\n",
       " 'whore',\n",
       " 'clich',\n",
       " 'also',\n",
       " 'skip',\n",
       " 'gangster',\n",
       " 'rapper',\n",
       " 'foul',\n",
       " 'languag',\n",
       " 'examin',\n",
       " 'sever',\n",
       " 'relationship',\n",
       " 'among',\n",
       " 'physician',\n",
       " 'african',\n",
       " 'american',\n",
       " 'african',\n",
       " 'american',\n",
       " 'male',\n",
       " 'femal',\n",
       " 'romant',\n",
       " 'relationship',\n",
       " 'complet',\n",
       " 'new',\n",
       " 'refresh',\n",
       " 'level',\n",
       " 'highli',\n",
       " 'impress',\n",
       " 'film',\n",
       " 'care',\n",
       " 'mix',\n",
       " 'light',\n",
       " 'head',\n",
       " 'humor',\n",
       " 'pretti',\n",
       " 'tough',\n",
       " 'heavi',\n",
       " 'issu',\n",
       " 'film',\n",
       " 'leav',\n",
       " 'feel',\n",
       " 'happi',\n",
       " 'sad',\n",
       " 'time',\n",
       " 'saw',\n",
       " 'boston',\n",
       " 'film',\n",
       " 'festiv',\n",
       " 'well',\n",
       " 'premier',\n",
       " 'amc',\n",
       " 'loew',\n",
       " 'boston',\n",
       " 'truli',\n",
       " 'hope',\n",
       " 'film',\n",
       " 'make',\n",
       " 'much',\n",
       " 'larger',\n",
       " 'audienc',\n",
       " 'black',\n",
       " 'peopl',\n",
       " 'everyon',\n",
       " 'els',\n",
       " 'would',\n",
       " 'love',\n",
       " 'movi',\n",
       " 'like',\n",
       " 'industri',\n",
       " 'smart',\n",
       " 'enough',\n",
       " 'put',\n",
       " 'extra',\n",
       " 'doctor',\n",
       " 'resid',\n",
       " 'medic',\n",
       " 'student',\n",
       " 'premed',\n",
       " 'student',\n",
       " 'marri',\n",
       " 'doctor',\n",
       " 'doctor',\n",
       " 'sibl',\n",
       " 'doctor',\n",
       " 'famili',\n",
       " 'know',\n",
       " 'great',\n",
       " 'doctor',\n",
       " 'period',\n",
       " 'would',\n",
       " 'love',\n",
       " 'film',\n",
       " 'well',\n",
       " 'portray',\n",
       " 'resid',\n",
       " 'practic',\n",
       " 'true',\n",
       " 'life',\n",
       " 'way',\n",
       " 'greatli',\n",
       " 'appreci',\n",
       " 'doctor',\n",
       " 'resid',\n",
       " 'medic',\n",
       " 'student',\n",
       " 'view',\n",
       " 'film',\n",
       " 'denni',\n",
       " 'cooper',\n",
       " 'complet',\n",
       " 'resid',\n",
       " 'intern',\n",
       " 'medicin',\n",
       " 'definit',\n",
       " 'appli',\n",
       " 'knowledg',\n",
       " 'experi',\n",
       " 'film',\n",
       " 'greatli',\n",
       " 'appar']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_to_words(train_X[100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By looking at this example, what does the method actually do? I'm going to bring everything together.\n",
    "\n",
    "It converts text to lowercase, transforms it into a list of words, removes stopwords (the most common words that don't carry much information on their own). It also removes punctuation from text so you don't analyze separately words like *clay* and *clay,* (with a comma). You can also see an example of the tokenization. Word *experi* represents the word *experience* from the review you saw before.\n",
    "\n",
    "Now that it is clear how it works, I'm going to apply it to the data. I will also save the result to a file. This operation can take a while. Next time, you can always read it from cache without needing to recode the text again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "cache_dir = os.path.join('./cache', 'sentiment_analysis')  # Where to store cache files\n",
    "os.makedirs(cache_dir, exist_ok = True)  # Ensure cache directory exists\n",
    "\n",
    "def preprocess_data(data_train, \n",
    "                    data_test, \n",
    "                    labels_train, \n",
    "                    labels_test,\n",
    "                    cache_dir=  cache_dir, \n",
    "                    cache_file = 'preprocessed_data.pkl'):\n",
    "    '''Convert each review to words. Read from cache if available.'''\n",
    "\n",
    "    # If cache_file is not None, try to read from it first\n",
    "    cache_data = None\n",
    "    if cache_file is not None:\n",
    "        try:\n",
    "            with open(os.path.join(cache_dir, cache_file), 'rb') as f:\n",
    "                cache_data = pickle.load(f)\n",
    "            print('Read preprocessed data from cache file:', cache_file)\n",
    "        except:\n",
    "            pass  # Unable to read from cache, but that's okay\n",
    "    \n",
    "    # If cache is missing, then do the heavy lifting\n",
    "    if cache_data is None:\n",
    "        # Preprocess training and test data to obtain words for each review\n",
    "        # words_train = list(map(review_to_words, data_train))\n",
    "        # words_test = list(map(review_to_words, data_test))\n",
    "        words_train = [review_to_words(review) for review in data_train]\n",
    "        words_test = [review_to_words(review) for review in data_test]\n",
    "        \n",
    "        # Write to cache file for future runs\n",
    "        if cache_file is not None:\n",
    "            cache_data = dict(words_train = words_train, \n",
    "                              words_test = words_test,\n",
    "                              labels_train = labels_train, \n",
    "                              labels_test = labels_test)\n",
    "            with open(os.path.join(cache_dir, cache_file), 'wb') as f:\n",
    "                pickle.dump(cache_data, f)\n",
    "            print('Wrote preprocessed data to cache file:', cache_file)\n",
    "    else:\n",
    "        # Unpack data loaded from cache file\n",
    "        words_train, words_test, labels_train, labels_test = (cache_data['words_train'], \n",
    "                                                              cache_data['words_test'], \n",
    "                                                              cache_data['labels_train'], \n",
    "                                                              cache_data['labels_test'])\n",
    "    \n",
    "    return words_train, words_test, labels_train, labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read preprocessed data from cache file: preprocessed_data.pkl\n"
     ]
    }
   ],
   "source": [
    "# Preprocess data\n",
    "train_X, test_X, train_y, test_y = preprocess_data(train_X, test_X, train_y, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform the data\n",
    "\n",
    "For the model I am going to build, I will construct a feature representation which is very similar to a bag-of words model. To start, I will represent each word as an integer. Of course, some of the words that appear in the reviews occur very infrequently and so likely don't contain much information for the purposes of sentiment analysis. The way you can deal with this problem is to fix the size of the working vocabulary and only include the words that appear most frequently. Then you will be able to combine all of the infrequent words into a single category and, in this case, I will label it as `1`.\n",
    "\n",
    "Since I will be using a recurrent neural network, it will be convenient if the length of each review is the same. To do this, I will fix a size for reviews and then pad short reviews with the category 'no word' (which I will label `0`) and truncate long reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a word dictionary\n",
    "\n",
    "To begin with, you need to find a way to map words that appear in the reviews to integers. Here I am going to fix the size of the vocabulary (including the 'no word' and 'infrequent' categories) to be 5000 but you may wish to change this to see how it affects the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def build_dict(data, vocab_size = 5000):\n",
    "    '''Construct and return a dictionary mapping each of the most frequently appearing words to a unique integer'''\n",
    "\n",
    "    # Determine how often each word appears in `data`\n",
    "    # `data` is a list of sentences and sentence is a list of words\n",
    "    words = []\n",
    "    for sentence in data:\n",
    "        for word in sentence:\n",
    "            words.append(word)\n",
    "    \n",
    "    # A dict storing the words that appear in the reviews along with how often they occur\n",
    "    word_count = dict(Counter(words))\n",
    "    \n",
    "    # Sort the words found in `data` so that sorted_words[0] is the most frequently appearing word and\n",
    "    # sorted_words[-1] is the least frequently appearing word.\n",
    "    sorted_words = sorted(word_count, key = word_count.get, reverse = True)\n",
    "    \n",
    "    word_dict = {} # This is going to be the end result, a dictionary that translates words into integers\n",
    "    for idx, word in enumerate(sorted_words[:vocab_size - 2]): # The -2 is so that there is a room for the \n",
    "                                                               # 'no word' and 'infrequent' labels\n",
    "    return word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict = build_dict(train_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the result to find out what are the five most frequent words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['movi', 'film', 'one', 'like', 'time']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import islice\n",
    "\n",
    "list(islice(word_dict, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By looking at these words, you can tell that it makes sense that they appear the most frequently in movie reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save `word_dict`\n",
    "\n",
    "Later on, the `word_dict` will be needed to encode the incoming reviews. User will send to the app a text, which needs to be transformed before being passed to the model to get a prediction. So I'm going to save it for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './data/pytorch' # The folder used for storing data\n",
    "if not os.path.exists(data_dir): # Make sure that the folder exists\n",
    "    os.makedirs(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(data_dir, 'word_dict.pkl'), 'wb') as f:\n",
    "    pickle.dump(word_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform the reviews\n",
    "\n",
    "Now I can use the dictionary to transform the words appearing in the reviews into integers. I will make sure to pad or truncate each one to a fixed length of 500."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_and_pad(word_dict, sentence, pad = 500):\n",
    "    NOWORD = 0 # Use 0 to represent the 'no word' category\n",
    "    INFREQ = 1 # Use 1 to represent the infrequent words, i.e., words not appearing in word_dict\n",
    "    \n",
    "    working_sentence = [NOWORD] * pad\n",
    "    \n",
    "    for word_index, word in enumerate(sentence[:pad]):\n",
    "        if word in word_dict:\n",
    "            working_sentence[word_index] = word_dict[word]\n",
    "        else:\n",
    "            working_sentence[word_index] = INFREQ\n",
    "            \n",
    "    return working_sentence, min(len(sentence), pad)\n",
    "\n",
    "def convert_and_pad_data(word_dict, data, pad = 500):\n",
    "    result = []\n",
    "    lengths = []\n",
    "    \n",
    "    for sentence in data:\n",
    "        converted, leng = convert_and_pad(word_dict, sentence, pad)\n",
    "        result.append(converted)\n",
    "        lengths.append(leng)\n",
    "        \n",
    "    return np.array(result), np.array(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, train_X_len = convert_and_pad_data(word_dict, train_X)\n",
    "test_X, test_X_len = convert_and_pad_data(word_dict, test_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a quick check, let's take a look at a one of transformed reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1052    1   90  172  849  102    3 4254    1 1062  476 2408 4828  897\n",
      "    1  405  811 1333  458   86  530  348 3739 3713 1688 4513    1 4180\n",
      " 1013  483 1072  279    1 1183  887    3  564  120 4919 1550    1  476\n",
      "  367 1062  476   28 1315  580    1  302  896  410   88  752  302    3\n",
      "  302   33    5    1    1    3 1063    1 1268  741  312 4977  917  520\n",
      "  191  475   38  549  121 1866  880  106    1  545  113  948  635   83\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0]\n",
      "\n",
      "length 500\n"
     ]
    }
   ],
   "source": [
    "print(train_X[100])\n",
    "print('\\nlength', len(train_X[100]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output looks reasonable. Most of the text is padded with `0` so it is rather a short review. Majority of the words seem to belong to a frequent category, while there is a few that belong to the infrequent category of `1`. It all makes sense. And finally, you can see that it keeps the desired length of 500.\n",
    "\n",
    "So the text above is processed using two functions, `preprocess_data` and `convert_and_pad_data`. Let's analyze what they actually do. `preprocess_data` applies `review_to_words` if it is run for the first time. In other case, it just reads the previous processing result from cache. And you could read what `review_to_works` does before. The only problem that I can see with this solution is that you can change your preprocessing code but without clearing the cache you won't see the results as it will still load the previous cache. So you need to keep this in mind.\n",
    "\n",
    "`convert_and_pad_data` does exactly what you can see above. It converts every word in a review to an integer. And there are two special ints. `0` used for padding and `1` for 'infrequent word' which simply appears less frequently than the top 4998 words. The potential disadvantage of this solution is that you process only 500 words from each review. In case of a longer review, you will loose some information as only first 500 words from it will be processed. All in all, I believe it is an optimization, as most reviews will be shorter than that. And you won't need to keep in memory all the additional `0` that would be used for padding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Copy the data to S3\n",
    "\n",
    "At now I was playing with data locally in the notebook. But SageMaker runs on AWS and it won't have access to any dataset until you upload to the cloud. The data is prepared now, so I will upload it to S3 instance.\n",
    "\n",
    "### Save the dataset locally\n",
    "\n",
    "I am going to save it locally first. Each row will start with `label`, `length` and then will come 500 integer-words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "    \n",
    "pd.concat([pd.DataFrame(train_y), pd.DataFrame(train_X_len), pd.DataFrame(train_X)], axis = 1) \\\n",
    "        .to_csv(os.path.join(data_dir, 'train.csv'), header = False, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload to the cloud\n",
    "\n",
    "Next, I can save it to the S3 bucket. I am just going to upload the entire `data` directory. It means that I will store not only the training set but also the word dictionary `word_dict.pkl`. It will come in handy later, when you will want to predict the sentiment on new data. The app will use it to process the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = 'sagemaker/sentiment_rnn'\n",
    "\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = sagemaker_session.upload_data(path=data_dir, bucket=bucket, key_prefix=prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Train the model\n",
    "\n",
    "The SageMaker model is made of three objects:\n",
    "\n",
    " - model artifacts,\n",
    " - training code,\n",
    " - inference code.\n",
    " \n",
    "Model artifacts result from training and they are objects like model weights. Code for training and inference, same as with data, needs to be packed into containers and uploaded to AWS.\n",
    "\n",
    "You can find the training code in the `train` directory. Let's take a look at `model.py`. It defines the neural network in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mnn\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnn\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mLSTMClassifier\u001b[39;49;00m(nn.Module):\r\n",
      "    \u001b[33m'''This is the simple RNN model for Sentiment Analysis'''\u001b[39;49;00m\r\n",
      "\r\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32m__init__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, embedding_dim, hidden_dim, vocab_size):\r\n",
      "        \u001b[33m'''Initialize the model and choose its parameters'''\u001b[39;49;00m\r\n",
      "        \u001b[36msuper\u001b[39;49;00m(LSTMClassifier, \u001b[36mself\u001b[39;49;00m).\u001b[32m__init__\u001b[39;49;00m()\r\n",
      "\r\n",
      "        \u001b[36mself\u001b[39;49;00m.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=\u001b[34m0\u001b[39;49;00m)\r\n",
      "        \u001b[36mself\u001b[39;49;00m.lstm = nn.LSTM(embedding_dim, hidden_dim)\r\n",
      "        \u001b[36mself\u001b[39;49;00m.dense = nn.Linear(in_features=hidden_dim, out_features=\u001b[34m1\u001b[39;49;00m)\r\n",
      "        \u001b[36mself\u001b[39;49;00m.sig = nn.Sigmoid()\r\n",
      "        \r\n",
      "        \u001b[36mself\u001b[39;49;00m.word_dict = \u001b[34mNone\u001b[39;49;00m\r\n",
      "\r\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mforward\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, x):\r\n",
      "        \u001b[33m'''Perform a forward pass on data x'''\u001b[39;49;00m\r\n",
      "        x = x.t()\r\n",
      "        lengths = x[\u001b[34m0\u001b[39;49;00m,:]\r\n",
      "        reviews = x[\u001b[34m1\u001b[39;49;00m:,:]\r\n",
      "        embeds = \u001b[36mself\u001b[39;49;00m.embedding(reviews)\r\n",
      "        lstm_out, _ = \u001b[36mself\u001b[39;49;00m.lstm(embeds)\r\n",
      "        out = \u001b[36mself\u001b[39;49;00m.dense(lstm_out)\r\n",
      "        out = out[lengths - \u001b[34m1\u001b[39;49;00m, \u001b[36mrange\u001b[39;49;00m(\u001b[36mlen\u001b[39;49;00m(lengths))]\r\n",
      "        \u001b[34mreturn\u001b[39;49;00m \u001b[36mself\u001b[39;49;00m.sig(out.squeeze())\r\n"
     ]
    }
   ],
   "source": [
    "!pygmentize train/model.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The important takeaway from the code is that there are three parameters that you may wish to fine tune later. I made them available in the constructor, so you can do it easily. They are the embedding dim, hidden dim and vocab size.\n",
    "\n",
    "I'm going to play with the code from `train.py` here. I want to test it before running SageMaker on a small set of only 250 reviews. I will load them first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data\n",
    "\n",
    "# Read in only the first 250 rows\n",
    "train_sample = pd.read_csv(os.path.join(data_dir, 'train.csv'), header = None, names = None, nrows = 250)\n",
    "\n",
    "# Turn the input pandas dataframe into tensors\n",
    "train_sample_y = torch.from_numpy(train_sample[[0]].values).float().squeeze()\n",
    "train_sample_X = torch.from_numpy(train_sample.drop([0], axis=1).values).long()\n",
    "\n",
    "# Build the dataset\n",
    "train_sample_ds = torch.utils.data.TensorDataset(train_sample_X, train_sample_y)\n",
    "# Build the dataloader\n",
    "train_sample_dl = torch.utils.data.DataLoader(train_sample_ds, batch_size = 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training method\n",
    "\n",
    "And now the code of training method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, epochs, optimizer, loss_fn, device):\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for batch in train_loader:         \n",
    "            batch_X, batch_y = batch\n",
    "            \n",
    "            batch_X = batch_X.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "            \n",
    "            # Clear the gradients\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass\n",
    "            output = model.forward(batch_X)\n",
    "            # Calculate the batch loss\n",
    "            loss = loss_fn(output, batch_y)\n",
    "            # Backward pass\n",
    "            loss.backward(retain_graph = True)\n",
    "            # Parameter update\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.data.item()\n",
    "            \n",
    "        print(\"Epoch: {}, BCELoss: {}\".format(epoch, total_loss / len(train_loader)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run it on the small dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, BCELoss: 0.6937276005744935\n",
      "Epoch: 2, BCELoss: 0.6814779281616211\n",
      "Epoch: 3, BCELoss: 0.6711473226547241\n",
      "Epoch: 4, BCELoss: 0.6603154182434082\n",
      "Epoch: 5, BCELoss: 0.6475648880004883\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from train.model import LSTMClassifier\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = LSTMClassifier(32, 100, 5000).to(device)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "loss_fn = torch.nn.BCELoss()\n",
    "\n",
    "train(model, train_sample_dl, 5, optimizer, loss_fn, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By looking at the results, it works perfectly.\n",
    "\n",
    "The last file that you can see is `requirements.txt`. It will be used to recreate the environment. You want to include there names of used packages like `numpy` and `nltk`. Now the training directory is complete and can be used to create a container."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model\n",
    "\n",
    "The SageMaker will execute the `train` package using the main method. I employ there an argument parser, because SageMaker passes hyperparameters this way.\n",
    "\n",
    "Now I'm going to use all that code and train the actual model. You can notice below that is just enough to pass to SageMaker the `train` directory and it will create a container itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "estimator = PyTorch(entry_point = 'train.py',\n",
    "                    source_dir = 'train',\n",
    "                    role = role,\n",
    "                    framework_version = '0.4.0',\n",
    "                    train_instance_count = 1,\n",
    "                    # train_instance_type='ml.p2.xlarge',\n",
    "                    # I use  this instance instead because my limit is 0 and I don't want to wait\n",
    "                    train_instance_type = 'ml.m4.xlarge',\n",
    "                    hyperparameters = {\n",
    "                        'epochs': 10,\n",
    "                        'hidden_dim': 200,\n",
    "                    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'create_image_uri' will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n",
      "'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\n",
      "'create_image_uri' will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-11-04 01:21:28 Starting - Starting the training job...\n",
      "2020-11-04 01:21:30 Starting - Launching requested ML instances......\n",
      "2020-11-04 01:22:54 Starting - Preparing the instances for training......\n",
      "2020-11-04 01:23:40 Downloading - Downloading input data...\n",
      "2020-11-04 01:24:23 Training - Training image download completed. Training in progress..\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2020-11-04 01:24:24,269 sagemaker-containers INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2020-11-04 01:24:24,272 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2020-11-04 01:24:24,286 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2020-11-04 01:24:24,290 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2020-11-04 01:24:24,500 sagemaker-containers INFO     Module train does not provide a setup.py. \u001b[0m\n",
      "\u001b[34mGenerating setup.py\u001b[0m\n",
      "\u001b[34m2020-11-04 01:24:24,501 sagemaker-containers INFO     Generating setup.cfg\u001b[0m\n",
      "\u001b[34m2020-11-04 01:24:24,501 sagemaker-containers INFO     Generating MANIFEST.in\u001b[0m\n",
      "\u001b[34m2020-11-04 01:24:24,501 sagemaker-containers INFO     Installing module with the following command:\u001b[0m\n",
      "\u001b[34m/usr/bin/python -m pip install -U . -r requirements.txt\u001b[0m\n",
      "\u001b[34mProcessing /opt/ml/code\u001b[0m\n",
      "\u001b[34mCollecting pandas (from -r requirements.txt (line 1))\n",
      "  Downloading https://files.pythonhosted.org/packages/74/24/0cdbf8907e1e3bc5a8da03345c23cbed7044330bb8f73bb12e711a640a00/pandas-0.24.2-cp35-cp35m-manylinux1_x86_64.whl (10.0MB)\u001b[0m\n",
      "\u001b[34mCollecting numpy (from -r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34m  Downloading https://files.pythonhosted.org/packages/b5/36/88723426b4ff576809fec7d73594fe17a35c27f8d01f93637637a29ae25b/numpy-1.18.5-cp35-cp35m-manylinux1_x86_64.whl (19.9MB)\u001b[0m\n",
      "\u001b[34mCollecting nltk (from -r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34m  Downloading https://files.pythonhosted.org/packages/92/75/ce35194d8e3022203cca0d2f896dbb88689f9b3fce8e9f9cff942913519d/nltk-3.5.zip (1.4MB)\u001b[0m\n",
      "\u001b[34mCollecting beautifulsoup4 (from -r requirements.txt (line 4))\n",
      "  Downloading https://files.pythonhosted.org/packages/d1/41/e6495bd7d3781cee623ce23ea6ac73282a373088fcd0ddc809a047b18eae/beautifulsoup4-4.9.3-py3-none-any.whl (115kB)\u001b[0m\n",
      "\u001b[34mCollecting html5lib (from -r requirements.txt (line 5))\n",
      "  Downloading https://files.pythonhosted.org/packages/6c/dd/a834df6482147d48e225a49515aabc28974ad5a4ca3215c18a882565b028/html5lib-1.1-py2.py3-none-any.whl (112kB)\u001b[0m\n",
      "\u001b[34mCollecting pytz>=2011k (from pandas->-r requirements.txt (line 1))\n",
      "  Downloading https://files.pythonhosted.org/packages/12/f8/ff09af6ff61a3efaad5f61ba5facdf17e7722c4393f7d8a66674d2dbd29f/pytz-2020.4-py2.py3-none-any.whl (509kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied, skipping upgrade: python-dateutil>=2.5.0 in /usr/local/lib/python3.5/dist-packages (from pandas->-r requirements.txt (line 1)) (2.7.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied, skipping upgrade: click in /usr/local/lib/python3.5/dist-packages (from nltk->-r requirements.txt (line 3)) (7.0)\u001b[0m\n",
      "\u001b[34mCollecting joblib (from nltk->-r requirements.txt (line 3))\n",
      "  Downloading https://files.pythonhosted.org/packages/28/5c/cf6a2b65a321c4a209efcdf64c2689efae2cb62661f8f6f4bb28547cf1bf/joblib-0.14.1-py2.py3-none-any.whl (294kB)\u001b[0m\n",
      "\u001b[34mCollecting regex (from nltk->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34m  Downloading https://files.pythonhosted.org/packages/fb/ef/8880ca0f2bc2e51a39aacf62a81947971aea42b3b54ca58111b5a5ba6e05/regex-2020.10.28.tar.gz (694kB)\u001b[0m\n",
      "\u001b[34mCollecting tqdm (from nltk->-r requirements.txt (line 3))\n",
      "  Downloading https://files.pythonhosted.org/packages/93/3a/96b3dc293aa72443cf9627444c3c221a7ba34bb622e4d8bf1b5d4f2d9d08/tqdm-4.51.0-py2.py3-none-any.whl (70kB)\u001b[0m\n",
      "\u001b[34mCollecting soupsieve>1.2; python_version >= \"3.0\" (from beautifulsoup4->-r requirements.txt (line 4))\n",
      "  Downloading https://files.pythonhosted.org/packages/6f/8f/457f4a5390eeae1cc3aeab89deb7724c965be841ffca6cfca9197482e470/soupsieve-2.0.1-py3-none-any.whl\u001b[0m\n",
      "\u001b[34mCollecting webencodings (from html5lib->-r requirements.txt (line 5))\n",
      "  Downloading https://files.pythonhosted.org/packages/f4/24/2a3e3df732393fed8b3ebf2ec078f05546de641fe1b667ee316ec1dcf3b7/webencodings-0.5.1-py2.py3-none-any.whl\u001b[0m\n",
      "\u001b[34mRequirement already satisfied, skipping upgrade: six>=1.9 in /usr/local/lib/python3.5/dist-packages (from html5lib->-r requirements.txt (line 5)) (1.11.0)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: nltk, train, regex\n",
      "  Running setup.py bdist_wheel for nltk: started\n",
      "  Running setup.py bdist_wheel for nltk: finished with status 'done'\n",
      "  Stored in directory: /root/.cache/pip/wheels/ae/8c/3f/b1fe0ba04555b08b57ab52ab7f86023639a526d8bc8d384306\u001b[0m\n",
      "\u001b[34m  Running setup.py bdist_wheel for train: started\n",
      "  Running setup.py bdist_wheel for train: finished with status 'done'\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-1p37xgps/wheels/35/24/16/37574d11bf9bde50616c67372a334f94fa8356bc7164af8ca3\n",
      "  Running setup.py bdist_wheel for regex: started\u001b[0m\n",
      "\u001b[34m  Running setup.py bdist_wheel for regex: finished with status 'done'\n",
      "  Stored in directory: /root/.cache/pip/wheels/16/32/f2/87817e06026bddd4f4f63c497c1c9587c028bdf57485e52e0f\u001b[0m\n",
      "\u001b[34mSuccessfully built nltk train regex\u001b[0m\n",
      "\u001b[34mInstalling collected packages: pytz, numpy, pandas, joblib, regex, tqdm, nltk, soupsieve, beautifulsoup4, webencodings, html5lib, train\n",
      "  Found existing installation: numpy 1.15.4\u001b[0m\n",
      "\u001b[34m    Uninstalling numpy-1.15.4:\n",
      "      Successfully uninstalled numpy-1.15.4\u001b[0m\n",
      "\u001b[34mSuccessfully installed beautifulsoup4-4.9.3 html5lib-1.1 joblib-0.14.1 nltk-3.5 numpy-1.18.5 pandas-0.24.2 pytz-2020.4 regex-2020.10.28 soupsieve-2.0.1 tqdm-4.51.0 train-1.0.0 webencodings-0.5.1\u001b[0m\n",
      "\u001b[34mYou are using pip version 18.1, however version 20.3b1 is available.\u001b[0m\n",
      "\u001b[34mYou should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[34m2020-11-04 01:24:46,411 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2020-11-04 01:24:46,425 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"module_dir\": \"s3://sagemaker-eu-central-1-406453037807/sagemaker-pytorch-2020-11-04-01-21-28-293/source/sourcedir.tar.gz\",\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"job_name\": \"sagemaker-pytorch-2020-11-04-01-21-28-293\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"hyperparameters\": {\n",
      "        \"epochs\": 10,\n",
      "        \"hidden_dim\": 200\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"log_level\": 20,\n",
      "    \"module_name\": \"train\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\",\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_HP_HIDDEN_DIM=200\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-eu-central-1-406453037807/sagemaker-pytorch-2020-11-04-01-21-28-293/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_HPS={\"epochs\":10,\"hidden_dim\":200}\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/usr/local/bin:/usr/lib/python35.zip:/usr/lib/python3.5:/usr/lib/python3.5/plat-x86_64-linux-gnu:/usr/lib/python3.5/lib-dynload:/usr/local/lib/python3.5/dist-packages:/usr/lib/python3/dist-packages\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"epochs\":10,\"hidden_dim\":200},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"job_name\":\"sagemaker-pytorch-2020-11-04-01-21-28-293\",\"log_level\":20,\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-eu-central-1-406453037807/sagemaker-pytorch-2020-11-04-01-21-28-293/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=10\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--epochs\",\"10\",\"--hidden_dim\",\"200\"]\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/usr/bin/python -m train --epochs 10 --hidden_dim 200\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34mUsing device cpu.\u001b[0m\n",
      "\u001b[34mGet train data loader.\u001b[0m\n",
      "\u001b[34mModel loaded with embedding_dim 32, hidden_dim 200, vocab_size 5000.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mEpoch: 1, BCELoss: 0.675865929953906\u001b[0m\n",
      "\u001b[34mEpoch: 2, BCELoss: 0.6087629515297559\u001b[0m\n",
      "\u001b[34mEpoch: 3, BCELoss: 0.5433617891097555\u001b[0m\n",
      "\u001b[34mEpoch: 4, BCELoss: 0.4614679637003918\u001b[0m\n",
      "\u001b[34mEpoch: 5, BCELoss: 0.4009826846268712\u001b[0m\n",
      "\u001b[34mEpoch: 6, BCELoss: 0.3666040392554536\u001b[0m\n",
      "\u001b[34mEpoch: 7, BCELoss: 0.35790663653490495\u001b[0m\n",
      "\u001b[34mEpoch: 8, BCELoss: 0.3162726644350558\u001b[0m\n",
      "\u001b[34mEpoch: 9, BCELoss: 0.2921612065057365\u001b[0m\n",
      "\n",
      "2020-11-04 03:04:03 Uploading - Uploading generated training model\u001b[34mEpoch: 10, BCELoss: 0.28056296128399516\u001b[0m\n",
      "\u001b[34m2020-11-04 03:03:59,558 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2020-11-04 03:04:09 Completed - Training job completed\n",
      "Training seconds: 6029\n",
      "Billable seconds: 6029\n"
     ]
    }
   ],
   "source": [
    "estimator.fit({'training': input_data})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Test the model\n",
    "\n",
    "As I said before, I will test the model two times, deploying it first.\n",
    "\n",
    "## Step 6: Deploy the model for testing\n",
    "\n",
    "To recap, the model takes as input vectors containing `review_length, review[500]` where `review[500]` is a sequence of `500` integer-words encoded with `word_dict`. SageMaker has in-built `predictor.predict()` function for models with simple input as this one. \n",
    "\n",
    "The only thing you need to provide is a function that loads the trained model. It must be called `model_fn()` and have only one parameter, a path to directory with the model. It also needs to be contained in the file that was used as `entry-point`. From the code above, you can see that I used `train.py`. If you want to take a look at my `model_fn()`, just go to this file.\n",
    "\n",
    "But first, let's deploy the model. After running the code below, SageMaker will launch a compute instance with it that can be accessed by using an endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter image will be renamed to image_uri in SageMaker Python SDK v2.\n",
      "'create_image_uri' will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------!"
     ]
    }
   ],
   "source": [
    "# Deploy the trained model\n",
    "predictor = estimator.deploy(initial_instance_count = 1, instance_type = 'ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Use the model for testing\n",
    "\n",
    "Now, you can send data to it and collect the predictions. I'm going to combine into a dataframe the `test_X` with info about its length. I will use this information in my local `predict` function that splits the dataset into an array of smaller arrays and then gathers results back. This trick will make sending data more efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X = pd.concat([pd.DataFrame(test_X_len), pd.DataFrame(test_X)], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into chunks and send each chunk seperately, accumulating the results\n",
    "\n",
    "def predict(data, rows=512):\n",
    "    split_array = np.array_split(data, int(data.shape[0] / float(rows) + 1))\n",
    "    predictions = np.array([])\n",
    "    for array in split_array:\n",
    "        predictions = np.append(predictions, predictor.predict(array))\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predict(test_X.values)\n",
    "predictions = [round(num) for num in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.85192"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(test_y, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy above 85% looks decent. And if you look at the training output, the model hasn't converged after the 10 training epochs it was trained for. It means that it can improve even further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More testing\n",
    "\n",
    "If you think about it, it would also be useful to see how the model performs with a previously unseen review. Ultimately, I would like to be able to send reviews in form of strings and get predictions for them. I am going to create a one for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_review = 'The simplest pleasures in life are the best, and this film is one of them. Combining a rather basic storyline of love and adventure this movie transcends the usual weekend fair with wit and unmitigated charm.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But the model can't directly work with text, it needs to preprocess and encode it as integers first. All you need to do is apply functions `review_to_words` and `convert_and_pad` that you saw before. Then just add the review length so you have `review_length, review[500]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert test_review into a form usable by the model and save the results in test_data\n",
    "review_words, length = convert_and_pad(word_dict, review_to_words(test_review))\n",
    "# create np array of the form [[length, review_words[0], review_words[1], ...]]\n",
    "test_data = np.array([np.array([length] + review_words)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model can process the review now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(0.8777149, dtype=float32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.predict(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the return value is close to `1`, it means that the prediction is 'positive'. That's right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete the endpoint\n",
    "\n",
    "The model passed all the tests. I can shut down the endpoint now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "estimator.delete_endpoint() will be deprecated in SageMaker Python SDK v2. Please use the delete_endpoint() function on your predictor instead.\n"
     ]
    }
   ],
   "source": [
    "estimator.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6 (again): Deploy the model for the web app\n",
    "\n",
    "As you could see, by default the model works if you pass to it preprocessed data. But it would be useful if it could somehow preprocess the review by itself. Then you could just send a raw review and receive a prediction.\n",
    "\n",
    "This is what inference code is meant to do. I am going to store it in the `serve` directory. SageMaker will then make a container out of it and use it when making predictions. It will contain a few files:\n",
    "\n",
    "- `model.py` having the same model code as in `train`,\n",
    "- `utils.py` containing `review_to_words` and `convert_and_pad` functions that will be used for pre-processing,\n",
    "- `predict.py` where I will put inference code,\n",
    "- `requirements.txt` that will tell what Python libraries needs to be provided so the code will work.\n",
    "\n",
    "I already have code for all the files but `predict.py`. What exactly does it need to contain? SageMaker expects it to have 4 methods:\n",
    "\n",
    " - `model_fn`: loads the model, the same as in `train`,\n",
    " - `input_fn`: takes the raw input and makes it available for the inference code,\n",
    " - `output_fn`: transforms the results before sending them back to the caller,\n",
    " - `predict_fn`: takes the input, does preprocessing, passes it to the model and returns predictions.\n",
    "\n",
    "For the simple app that I am building here, `input_fn` and `output_fn` don't need to do much. The former is going to decode text from `utf-8` format used by a webpage so it is a plain text. The latter is just going to return the model output. It would take much more work to deserialize and serialize the image data though.\n",
    "\n",
    "### Write the inference code\n",
    "\n",
    "Let's complete the `predict.py` and write code for the four functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mjson\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpickle\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msys\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msagemaker_containers\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpandas\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mpd\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mnumpy\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnp\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mnn\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnn\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36moptim\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36moptim\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mutils\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mdata\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mmodel\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m LSTMClassifier\r\n",
      "\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mutils\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m review_to_words, convert_and_pad\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mmodel_fn\u001b[39;49;00m(model_dir):\r\n",
      "    \u001b[33m'''Load the PyTorch model from the directory'''\u001b[39;49;00m\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mLoading model.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "\r\n",
      "    \u001b[37m# First, load the parameters used to create the model.\u001b[39;49;00m\r\n",
      "    model_info = {}\r\n",
      "    model_info_path = os.path.join(model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mmodel_info.pth\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(model_info_path, \u001b[33m'\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\r\n",
      "        model_info = torch.load(f)\r\n",
      "\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mmodel_info: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(model_info))\r\n",
      "\r\n",
      "    \u001b[37m# Determine the device and construct the model.\u001b[39;49;00m\r\n",
      "    device = torch.device(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[34mif\u001b[39;49;00m torch.cuda.is_available() \u001b[34melse\u001b[39;49;00m \u001b[33m'\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    model = LSTMClassifier(model_info[\u001b[33m'\u001b[39;49;00m\u001b[33membedding_dim\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], model_info[\u001b[33m'\u001b[39;49;00m\u001b[33mhidden_dim\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], model_info[\u001b[33m'\u001b[39;49;00m\u001b[33mvocab_size\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\r\n",
      "\r\n",
      "    \u001b[37m# Load the stored model parameters.\u001b[39;49;00m\r\n",
      "    model_path = os.path.join(model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mmodel.pth\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(model_path, \u001b[33m'\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\r\n",
      "        model.load_state_dict(torch.load(f))\r\n",
      "\r\n",
      "    \u001b[37m# Load the saved word_dict.\u001b[39;49;00m\r\n",
      "    word_dict_path = os.path.join(model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mword_dict.pkl\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(word_dict_path, \u001b[33m'\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\r\n",
      "        model.word_dict = pickle.load(f)\r\n",
      "\r\n",
      "    model.to(device).eval()\r\n",
      "\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mDone loading model.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m model\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32minput_fn\u001b[39;49;00m(serialized_input_data, content_type):\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mDeserializing the input data.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \u001b[34mif\u001b[39;49;00m content_type == \u001b[33m'\u001b[39;49;00m\u001b[33mtext/plain\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\r\n",
      "        data = serialized_input_data.decode(\u001b[33m'\u001b[39;49;00m\u001b[33mutf-8\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "        \u001b[34mreturn\u001b[39;49;00m data\r\n",
      "    \u001b[34mraise\u001b[39;49;00m \u001b[36mException\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mRequested unsupported ContentType in content_type: \u001b[39;49;00m\u001b[33m'\u001b[39;49;00m + content_type)\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32moutput_fn\u001b[39;49;00m(prediction_output, accept):\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mSerializing the generated output.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m \u001b[36mstr\u001b[39;49;00m(prediction_output)\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mpredict_fn\u001b[39;49;00m(input_data, model):\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mInferring sentiment of input data.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "\r\n",
      "    device = torch.device(\u001b[33m'\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m \u001b[34mif\u001b[39;49;00m torch.cuda.is_available() \u001b[34melse\u001b[39;49;00m \u001b[33m'\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \r\n",
      "    \u001b[34mif\u001b[39;49;00m model.word_dict \u001b[35mis\u001b[39;49;00m \u001b[34mNone\u001b[39;49;00m:\r\n",
      "        \u001b[34mraise\u001b[39;49;00m \u001b[36mException\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mModel has not been loaded properly, no word_dict.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \r\n",
      "    \u001b[37m# Process input_data so that it is ready to be sent to the model.\u001b[39;49;00m\r\n",
      "    \u001b[37m# Produce two variables:\u001b[39;49;00m\r\n",
      "    \u001b[37m#     data_X   - review converted to 500 integers\u001b[39;49;00m\r\n",
      "    \u001b[37m#     data_len - review length\u001b[39;49;00m\r\n",
      "\r\n",
      "    data_X, data_len = convert_and_pad(model.word_dict, review_to_words(input_data))\r\n",
      "\r\n",
      "    \u001b[37m# Create input tensor of the form 'len, review[500]'\u001b[39;49;00m\r\n",
      "    data_pack = np.hstack((data_len, data_X))\r\n",
      "    data_pack = data_pack.reshape(\u001b[34m1\u001b[39;49;00m, -\u001b[34m1\u001b[39;49;00m)\r\n",
      "    \r\n",
      "    data = torch.from_numpy(data_pack)\r\n",
      "    data = data.to(device)\r\n",
      "\r\n",
      "    \u001b[37m# Remember to put the model into evaluation mode\u001b[39;49;00m\r\n",
      "    model.eval()\r\n",
      "\r\n",
      "    \u001b[37m# Compute the result by running a forward pass, it should be 0 or 1\u001b[39;49;00m\r\n",
      "    \u001b[34mwith\u001b[39;49;00m torch.no_grad():\r\n",
      "        output = model.forward(data)\r\n",
      "\r\n",
      "    result = np.round(output.numpy()).astype(\u001b[36mint\u001b[39;49;00m)\r\n",
      "\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m result\r\n"
     ]
    }
   ],
   "source": [
    "!pygmentize serve/predict.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploying the model\n",
    "\n",
    "The inference code is ready and I can deploy the model. But first, I need to construct a new PyTorchModel and make sure that it points to right elements. These are the model artifacts that result from training, path to directory with inference code and its entry point being `predict.py`.\n",
    "\n",
    "By default, a deployed PyTorch model assumes that any input takes the form of a `numpy` array. In this case, I want the container to take in review strings, so I wrap the predictor class with `StringPredictor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter image will be renamed to image_uri in SageMaker Python SDK v2.\n",
      "'create_image_uri' will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------!"
     ]
    }
   ],
   "source": [
    "from sagemaker.predictor import RealTimePredictor\n",
    "from sagemaker.pytorch import PyTorchModel\n",
    "\n",
    "class StringPredictor(RealTimePredictor):\n",
    "    def __init__(self, endpoint_name, sagemaker_session):\n",
    "        super(StringPredictor, self).__init__(endpoint_name, sagemaker_session, content_type = 'text/plain')\n",
    "\n",
    "model = PyTorchModel(model_data = estimator.model_data,\n",
    "                     role = role,\n",
    "                     framework_version = '0.4.0',\n",
    "                     entry_point = 'predict.py',\n",
    "                     source_dir = 'serve',\n",
    "                     predictor_cls = StringPredictor)\n",
    "predictor = model.deploy(initial_instance_count = 1, instance_type = 'ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the model\n",
    "\n",
    "Let's test the deployed model by sending to it 250 positive and 250 negative reviews. I won't send the entire testing set though. The model processes review one by one and both the input and output need to be sent through the Internet, so it would take quite a long time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "def test_reviews(data_dir='../data/aclImdb', stop = 250):\n",
    "    \n",
    "    results = []\n",
    "    ground = []\n",
    "    \n",
    "    # Test both positive and negative reviews    \n",
    "    for sentiment in ['pos', 'neg']:\n",
    "        \n",
    "        path = os.path.join(data_dir, 'test', sentiment, '*.txt')\n",
    "        files = glob.glob(path)\n",
    "        \n",
    "        files_read = 0\n",
    "        \n",
    "        print('Starting ', sentiment, ' files')\n",
    "        \n",
    "        # Iterate through the files and send them to the predictor\n",
    "        for f in files:\n",
    "            with open(f) as review:\n",
    "                # First store the true label\n",
    "                if sentiment == 'pos':\n",
    "                    ground.append(1)\n",
    "                else:\n",
    "                    ground.append(0)\n",
    "                # Read in the review and convert to 'utf-8' for transmission via HTTP\n",
    "                review_input = review.read().encode('utf-8')\n",
    "                # Send the review to the predictor and store the results\n",
    "                results.append(int(predictor.predict(review_input)))\n",
    "                \n",
    "            # Sending reviews to the endpoint one at a time takes a while\n",
    "            # So stop after reaching 'stop' value\n",
    "            files_read += 1\n",
    "            if files_read == stop:\n",
    "                break\n",
    "            \n",
    "    return ground, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting  pos  files\n",
      "Starting  neg  files\n"
     ]
    }
   ],
   "source": [
    "ground, results = test_reviews()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.838"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(ground, results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is more less the same as on the entire testing set. Some difference is expected, because the subset can for example contain more difficult inputs on average compared to the whole set.\n",
    "\n",
    "As an additional test, you can send the `test_review` that I used earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'1'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.predict(test_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It again predicts the correct, positive sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7 (again): Use the model for the web app\n",
    "\n",
    "All the time I have been interacting with the deployed model from this notebook which has a proper IAM role allowing for seamless communication. If I wanted to communicate with the model from a webpage, the situation would be different. In order to access the SageMaker, the app would have to authenticate with AWS every time. However, I can still enable seamless communication between the app and the SageMaker using AWS Lambda.\n",
    "\n",
    "<img src=\"images/web_app_diagram.svg\">\n",
    "\n",
    "You can see the architecture of this solution on the diagram above. The main point is the Lambda. You can think of it as a python function that is executed reactively. What I mean by this is that it will have its own endpoint, a url created using API Gateway. Once the app sends data to this endpoint, it will trigger the Lambda. The Lambda, exisiting on AWS, will directly access the model, ask it for prediction and then send it back to the app. Actually, the Lambda acts as an intermediary between the app and the model and it can access it without authentication.\n",
    "\n",
    "### Set up a Lambda function\n",
    "\n",
    "#### Part A: Create an IAM Role for the Lambda function\n",
    "\n",
    "Since you want the Lambda function to call a SageMaker endpoint, it needs a proper IAM role to do so. IAM role is the same as permission. If you want to create it on your own, you can follow the steps below.\n",
    "\n",
    "Using the AWS Console, navigate to the **IAM** page and click on **Roles**. Then, click on **Create role**. Make sure that the **AWS service** is the type of trusted entity selected and choose **Lambda** as the service that will use this role, then click **Next: Permissions**.\n",
    "\n",
    "In the search box type `sagemaker` and select the check box next to the **AmazonSageMakerFullAccess** policy. Then, click on **Next: Review**.\n",
    "\n",
    "Lastly, give this role a name. Make sure you use a name that you will remember later on, for example `LambdaSageMakerRole`. Then, click on **Create role**.\n",
    "\n",
    "#### Part B: Create a Lambda function\n",
    "\n",
    "Now it is time to actually create the Lambda function. Again, if you want to do it yourself, instructions are below.\n",
    "\n",
    "Using the AWS Console, navigate to the AWS Lambda page and click on **Create a function**. When you get to the next page, make sure that **Author from scratch** is selected. Now, name your Lambda function, using a name that you will remember later on, for example `sentiment_analysis_func`. Make sure that the **Python 3.6** runtime is selected and then choose the role that you created in the previous part. Then, click on **Create Function**.\n",
    "\n",
    "On the next page you will see some information about the Lambda function you've just created. If you scroll down you should see an editor in which you can write the code that will be executed when your Lambda function is triggered. For this project, I will use the code below.\n",
    "\n",
    "```python\n",
    "# Use the low-level library to interact with SageMaker since the SageMaker API\n",
    "# is not available natively through Lambda.\n",
    "import boto3\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "\n",
    "    # To invoke the endpoint, you need the SageMaker runtime\n",
    "    runtime = boto3.Session().client('sagemaker-runtime')\n",
    "\n",
    "    # Use that runtime to invoke the model's endpoint, by sending the review received from the app\n",
    "    response = runtime.invoke_endpoint(EndpointName = '**ENDPOINT NAME HERE**',    # Name of the created endpoint\n",
    "                                       ContentType = 'text/plain',                 # The data format expected by the endpoint\n",
    "                                       Body = event['body'])                       # The actual review\n",
    "\n",
    "    # The result is an HTTP response, the body contains the prediction\n",
    "    result = response['Body'].read().decode('utf-8')\n",
    "\n",
    "    return {\n",
    "        'statusCode' : 200,\n",
    "        'headers' : { 'Content-Type' : 'text/plain', 'Access-Control-Allow-Origin' : '*' },\n",
    "        'body' : result\n",
    "    }\n",
    "```\n",
    "\n",
    "Once you have copy and pasted the code above into the Lambda code editor, replace the `**ENDPOINT NAME HERE**` portion with the name of the model's endpoint. You can get it by running the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sagemaker-pytorch-2020-11-04-04-24-19-257'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have added the endpoint name to the Lambda function, click on **Save**. Your Lambda function is now up and running. \n",
    "\n",
    "### Set up API Gateway\n",
    "\n",
    "Now that the Lambda function is set up, it's time to create an endpoint for it using API Gateway. This url will be used to trigger the Lambda.\n",
    "\n",
    "Using AWS Console, navigate to **Amazon API Gateway** and then click on **Get started**.\n",
    "\n",
    "On the next page, make sure that **New API** is selected and give the new api a name, for example, `sentiment_analysis_api`. Then, click on **Create API**.\n",
    "\n",
    "Now you created an API, however it doesn't currently do anything. What you want it to do is to trigger the Lambda function that you created earlier.\n",
    "\n",
    "Select the **Actions** dropdown menu and click **Create Method**. A new blank method will be created, select its dropdown menu and select **POST**, then click on the check mark beside it.\n",
    "\n",
    "For the integration point, make sure that **Lambda Function** is selected and click on the **Use Lambda Proxy integration**. This option makes sure that the data that is sent to the API is then sent directly to the Lambda function with no processing. It also means that the return value must be a proper response object as it will also not be processed by API Gateway.\n",
    "\n",
    "Type the name of the Lambda function you created earlier into the **Lambda Function** text entry box and then click on **Save**. Click on **OK** in the pop-up box that then appears, giving permission to API Gateway to invoke the Lambda function you created.\n",
    "\n",
    "The last step in creating the API Gateway is to select the **Actions** dropdown and click on **Deploy API**. You will need to create a new Deployment stage and name it anything you like, for example `prod`.\n",
    "\n",
    "You have now successfully set up a public API to access your SageMaker model. Make sure to copy or write down the URL provided to invoke your newly created public API as this will be needed in the next step. This URL can be found at the top of the page, highlighted in blue next to the text **Invoke URL**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Deploy the web app\n",
    "\n",
    "Now that I have API, Lambda and the model up and running, the last step is to prepare a webpage. I am going to use a simple static html file. It will communicate with the model through the publicly available API endpoint.\n",
    "\n",
    "In the `website` folder, there is a file called `index.html`. All you need to do is download it on your computer  and open it in a text editor. Next, find the comment **\\*\\*REPLACE WITH PUBLIC API URL\\*\\*** and replace the url that I use below it with your url from the previous step.\n",
    "\n",
    "Now, if you open this file in your browser, it will behave as a local web server. You can use it to interact with your SageMaker model. You can play with it for a while, but remember to shut down the endpoint when you finish. Amazon will charge you for the time it is running.\n",
    "\n",
    "I tested the app by submitting a review *Masterpiece. Everyone should see it!*. It predicted postive sentiment, so it works correctly. You can see the screenshot below.\n",
    "\n",
    "<img src=\"images/web_app_example.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete the endpoint\n",
    "\n",
    "Remember to always shut down your endpoint if you are no longer using it. You are charged for the length of time that the endpoint is running so if you forget and leave it on you could end up with an unexpectedly large bill."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit ('pytorch_env': conda)",
   "language": "python",
   "name": "python37764bitpytorchenvconda85ef0111a8ff4a73a0db5833bf0fd60c"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
